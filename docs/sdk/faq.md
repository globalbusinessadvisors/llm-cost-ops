# Frequently Asked Questions (FAQ)

Answers to common questions about LLM-CostOps.

## General Questions

### What is LLM-CostOps?

LLM-CostOps is an enterprise-grade cost operations platform for LLM infrastructure. It helps you track, analyze, and optimize costs across multiple LLM providers (OpenAI, Anthropic, Google, Azure, AWS, and more) with production-ready accuracy.

### Is LLM-CostOps open source?

Yes! LLM-CostOps is dual-licensed under Apache 2.0 / MIT. The core platform is fully open source and can be self-hosted.

### What's the difference between cloud and self-hosted?

| Feature | Cloud (Hosted) | Self-Hosted |
|---------|----------------|-------------|
| Setup | Instant | Manual deployment |
| Maintenance | Managed by us | You manage |
| Scalability | Auto-scaling | Configure yourself |
| Cost | Monthly subscription | Infrastructure costs only |
| Data Location | Our servers | Your infrastructure |
| Customization | Limited | Full control |

### Which LLM providers are supported?

Currently supported providers:
- **OpenAI** (GPT-4, GPT-3.5, etc.)
- **Anthropic** (Claude 3 family)
- **Google Vertex AI** (Gemini, PaLM)
- **Azure OpenAI**
- **AWS Bedrock**
- **Cohere**
- **Mistral AI**
- **Custom providers** (you can add your own)

### How accurate are the cost calculations?

LLM-CostOps uses 10-decimal precision (rust_decimal) for accurate financial calculations. Costs are calculated based on:
- Exact token counts from provider responses
- Up-to-date pricing tables
- Support for tiered pricing and volume discounts
- Cached token discounts (where applicable)

### Can I track costs across multiple organizations?

Yes! LLM-CostOps supports multi-tenancy with organization-level isolation. Each organization has:
- Separate cost tracking
- Independent API keys
- Individual budgets and alerts
- Role-based access control (RBAC)

---

## Pricing & Billing

### How much does LLM-CostOps cost?

**Cloud (Hosted):**
- **Free Tier**: Up to 1M tokens/month
- **Starter**: $29/month (10M tokens)
- **Professional**: $99/month (100M tokens)
- **Enterprise**: Custom pricing (unlimited)

**Self-Hosted:**
- Free (just infrastructure costs)
- Optional premium support available

### How is usage calculated?

LLM-CostOps tracks:
- **Input tokens**: Tokens sent to the LLM
- **Output tokens**: Tokens generated by the LLM
- **Cached tokens**: Tokens from prompt caching (if supported)
- **Total requests**: Number of API calls

Costs are calculated as:
```
cost = (input_tokens / 1000 * input_price) +
       (output_tokens / 1000 * output_price)
```

### Can I set spending limits?

Yes! You can set:
- **Organization budgets**: Total monthly spend limit
- **Project budgets**: Per-project spend limits
- **User budgets**: Per-user spend limits
- **Model budgets**: Per-model spend limits

Alerts can be sent at customizable thresholds (e.g., 50%, 80%, 100%).

### What happens if I exceed my budget?

It depends on your configuration:
- **Soft limit**: You receive alerts but API continues working
- **Hard limit**: API requests are blocked when budget is exceeded
- **Rate limiting**: Requests are throttled after threshold

---

## Technical Questions

### Which programming languages have SDKs?

**Current Status:**
- âœ… **REST API**: Available now (works with any language)
- ðŸ”„ **Python**: Planned
- ðŸ”„ **TypeScript/JavaScript**: Planned
- ðŸ”„ **Go**: Planned
- ðŸ”„ **Java**: Planned

Until official SDKs are available, use the REST API directly (see [cURL examples](examples/curl/)).

### How do I track LLM usage automatically?

**Option 1: Manual Tracking**
```python
response = openai.ChatCompletion.create(...)

# Track usage
cost_ops.usage.submit(
    organization_id="org-123",
    provider="openai",
    model_id="gpt-4",
    input_tokens=response.usage.prompt_tokens,
    output_tokens=response.usage.completion_tokens,
    total_tokens=response.usage.total_tokens
)
```

**Option 2: Middleware** (recommended)
```python
# FastAPI example
from llm_cost_ops.middleware import CostTrackingMiddleware

app.add_middleware(
    CostTrackingMiddleware,
    api_key=os.getenv("LLM_COST_OPS_API_KEY")
)
```

**Option 3: Proxy** (coming soon)
- Route all LLM requests through LLM-CostOps proxy
- Automatic tracking for all providers
- No code changes required

### Can I export my cost data?

Yes! Multiple export formats:
- **CSV**: For Excel, Google Sheets
- **JSON**: For custom processing
- **Excel (XLSX)**: With charts and pivot tables
- **JSON Lines**: For streaming/big data
- **Parquet**: For data warehouses

```bash
# Export to CSV
curl -X GET "$BASE_URL/api/v1/costs/export?format=csv&start_date=2025-01-01&end_date=2025-01-31" \
  -H "Authorization: Bearer $API_KEY" \
  -o costs.csv
```

### How do I integrate with my existing tools?

**BI Tools:**
- Tableau, Power BI, Looker: Use CSV/Excel export
- Metabase, Redash: Connect to PostgreSQL database (self-hosted)

**Monitoring:**
- Datadog, New Relic: Use Prometheus metrics endpoint
- Grafana: Use pre-built dashboards

**Communication:**
- Slack, Teams: Use webhook integrations
- Email: Scheduled reports

**CI/CD:**
- GitHub Actions, GitLab CI: Use REST API in pipelines
- Jenkins: Use curl or SDK in build scripts

### What metrics are available?

**Cost Metrics:**
- Total cost (by period, provider, model, project)
- Average cost per request
- Cost per 1K tokens
- Cost trends over time

**Usage Metrics:**
- Total tokens (input, output, cached)
- Request count
- Average tokens per request
- Token distribution

**Performance Metrics:**
- Request latency
- Error rates
- Provider availability
- Cache hit rates

**Business Metrics:**
- Cost per user
- Cost per feature
- ROI by use case
- Budget utilization

---

## Security & Compliance

### Is my data secure?

Yes! Security measures include:
- **Encryption in transit**: TLS 1.3 for all API requests
- **Encryption at rest**: AES-256 for database (cloud)
- **API key hashing**: SHA-256 hashed keys
- **Audit logging**: All actions logged with timestamps
- **RBAC**: Fine-grained permission control
- **Multi-tenancy**: Complete data isolation between organizations

### Where is my data stored?

**Cloud (Hosted):**
- Primary: US East (AWS)
- Europe region available (GDPR compliant)
- Data residency options for enterprise customers

**Self-Hosted:**
- Your infrastructure (full control)
- Any cloud provider (AWS, GCP, Azure, etc.)
- On-premises deployment supported

### Is LLM-CostOps GDPR compliant?

Yes! For cloud customers:
- Data Processing Agreement (DPA) available
- EU data residency option
- Right to erasure (data deletion)
- Data portability (exports)
- Privacy by design

Self-hosted customers have full control over compliance.

### What data does LLM-CostOps collect?

LLM-CostOps only collects:
- **Usage metadata**: Token counts, model, provider, timestamp
- **Cost data**: Calculated costs (no LLM prompts/responses)
- **Custom metadata**: Any tags you add (optional)

**We do NOT collect:**
- LLM prompts or user inputs
- LLM responses or outputs
- End-user personal data (unless you add it)
- API keys for LLM providers (we never see these)

### Can I use LLM-CostOps in production?

Yes! LLM-CostOps is production-ready:
- âœ… High availability (99.9% uptime SLA for cloud)
- âœ… Horizontal scaling
- âœ… Battle-tested with billions of tokens
- âœ… Comprehensive error handling
- âœ… Monitoring and observability
- âœ… Zero-downtime deployments

---

## Features & Capabilities

### Can I track costs by user, project, or team?

Yes! Use the `metadata` field to add custom dimensions:

```python
client.usage.submit(
    organization_id="org-123",
    provider="openai",
    model_id="gpt-4",
    input_tokens=1000,
    output_tokens=500,
    total_tokens=1500,
    metadata={
        "user_id": "user-456",
        "project": "chatbot",
        "team": "customer-support",
        "environment": "production",
        "customer_tier": "enterprise"
    }
)
```

Then query by metadata:
```bash
curl -X GET "$BASE_URL/api/v1/costs?metadata.team=customer-support"
```

### Does LLM-CostOps support cost forecasting?

Yes! (Coming soon)

Features:
- **Time-series forecasting**: Predict future costs
- **Trend analysis**: Identify cost trends
- **Anomaly detection**: Alert on unusual spikes
- **Budget forecasting**: Predict when budget will be exceeded
- **Confidence intervals**: Understand forecast uncertainty

### Can I set up automatic alerts?

Yes! Alert types:
- **Budget alerts**: When spending exceeds thresholds
- **Anomaly alerts**: Unusual cost spikes
- **Usage alerts**: High token usage
- **Error alerts**: API failures

Delivery channels:
- Email
- Slack
- Microsoft Teams
- PagerDuty
- Webhooks (custom integrations)

### How does rate limiting work?

**Cloud (Hosted):**
- Free: 10 requests/minute
- Starter: 100 requests/minute
- Professional: 1,000 requests/minute
- Enterprise: Custom limits

**Self-Hosted:**
- Configure your own limits
- Default: 100 requests/minute per API key

Rate limit headers:
```
X-RateLimit-Limit: 100
X-RateLimit-Remaining: 95
X-RateLimit-Reset: 1705320060
```

### Can I analyze historical data?

Yes! LLM-CostOps stores all historical usage:
- No data retention limits
- Query any date range
- Time-series analytics
- Trend analysis
- Year-over-year comparisons

---

## Deployment & Operations

### How do I deploy LLM-CostOps?

**Cloud (Hosted):**
1. Sign up at https://app.llm-cost-ops.dev
2. Get API key
3. Start tracking usage

**Self-Hosted:**

**Docker:**
```bash
docker run -d \
  -p 8080:8080 \
  -e DATABASE_URL=postgresql://user:pass@db/costops \
  llm-cost-ops/api:latest
```

**Kubernetes:**
```bash
helm install llm-cost-ops ./k8s/helm/llm-cost-ops
```

**Binary:**
```bash
cargo build --release
./target/release/cost-ops init
./target/release/cost-ops serve
```

### What are the system requirements?

**Minimum (Small deployment):**
- 1 CPU core
- 512 MB RAM
- 10 GB storage
- PostgreSQL 13+ or SQLite

**Recommended (Production):**
- 4 CPU cores
- 4 GB RAM
- 100 GB storage (SSD)
- PostgreSQL 14+
- Redis (for caching)

**High availability:**
- 3+ replicas
- Load balancer
- PostgreSQL with replication
- Redis cluster

### Can I run LLM-CostOps on AWS/GCP/Azure?

Yes! LLM-CostOps works on any cloud:
- **AWS**: ECS, EKS, EC2
- **GCP**: GKE, Cloud Run, Compute Engine
- **Azure**: AKS, Container Apps, VMs
- **DigitalOcean**: Kubernetes, Droplets
- **On-premises**: Any Kubernetes or VM

### How do I backup my data?

**Self-Hosted:**

**Database backups:**
```bash
# PostgreSQL
pg_dump -h localhost -U postgres costops > backup.sql

# Restore
psql -h localhost -U postgres costops < backup.sql
```

**Automated backups:**
```bash
# Use cron
0 2 * * * /usr/local/bin/backup-costops.sh
```

**Cloud (Hosted):**
- Automatic daily backups
- 30-day retention
- Point-in-time recovery
- Download backups anytime

### How do I upgrade to a new version?

**Cloud (Hosted):**
- Automatic updates
- Zero downtime
- No action required

**Self-Hosted:**

**Docker:**
```bash
docker pull llm-cost-ops/api:latest
docker stop llm-cost-ops
docker rm llm-cost-ops
docker run -d ... llm-cost-ops/api:latest
```

**Kubernetes:**
```bash
helm upgrade llm-cost-ops ./k8s/helm/llm-cost-ops
```

**Binary:**
```bash
cargo build --release
./target/release/cost-ops migrate  # Run migrations
systemctl restart llm-cost-ops
```

---

## Support & Community

### How do I get support?

**Free/Self-Hosted:**
- Documentation: https://docs.llm-cost-ops.dev
- GitHub Issues: https://github.com/llm-devops/llm-cost-ops/issues
- Discord Community: https://discord.gg/llm-cost-ops

**Paid Cloud:**
- Email support: support@llm-cost-ops.dev
- Priority support (Professional/Enterprise)
- Dedicated Slack channel (Enterprise)

### Can I contribute to LLM-CostOps?

Yes! We welcome contributions:
- Report bugs
- Suggest features
- Submit pull requests
- Improve documentation
- Share use cases

See [CONTRIBUTING.md](../../CONTRIBUTING.md) for guidelines.

### Where can I find examples and tutorials?

- [Quickstart Guide](getting-started/quickstart.md)
- [cURL Examples](examples/curl/)
- [Python Examples](examples/python/)
- [Framework Integrations](frameworks/)
- [Video Tutorials](https://www.youtube.com/llm-cost-ops) (coming soon)

### Is there a community forum?

Yes!
- **Discord**: https://discord.gg/llm-cost-ops
- **GitHub Discussions**: https://github.com/llm-devops/llm-cost-ops/discussions
- **Reddit**: r/llm-cost-ops

### How often is LLM-CostOps updated?

- **Minor releases**: Every 2 weeks
- **Major releases**: Every 2-3 months
- **Security patches**: As needed (immediate)
- **Pricing updates**: Monthly (or when providers change pricing)

---

## Advanced Topics

### Can I build custom dashboards?

Yes! Options:
1. **Use our API** to fetch data and build custom dashboards
2. **Direct database access** (self-hosted) for BI tools
3. **Prometheus metrics** for Grafana dashboards
4. **Export data** to your own data warehouse

### Does LLM-CostOps support webhooks?

Yes! Receive events via webhooks:
- Usage submitted
- Budget threshold exceeded
- Cost anomaly detected
- Pricing updated

Configure webhooks:
```bash
curl -X POST $BASE_URL/api/v1/webhooks \
  -H "Authorization: Bearer $API_KEY" \
  -d '{
    "url": "https://your-app.com/webhook",
    "events": ["budget.threshold", "cost.anomaly"],
    "secret": "your-webhook-secret"
  }'
```

### Can I customize pricing models?

Yes! LLM-CostOps supports:
- **Per-token pricing**: Standard model
- **Per-request pricing**: Fixed cost per call
- **Tiered pricing**: Volume discounts
- **Custom pricing**: Define your own formula

### How do I migrate from another tool?

1. **Export data** from your current tool (CSV, JSON)
2. **Transform data** to LLM-CostOps format
3. **Import via API**:
   ```bash
   curl -X POST $BASE_URL/api/v1/usage/batch \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer $API_KEY" \
     -d @usage-data.json
   ```

We can help with migration for Enterprise customers.

---

## Still have questions?

- **Email**: support@llm-cost-ops.dev
- **Discord**: https://discord.gg/llm-cost-ops
- **Schedule a call**: https://llm-cost-ops.dev/contact

## Next Steps

- [Get Started with the Quickstart Guide](getting-started/quickstart.md)
- [Read the Full Documentation](README.md)
- [Join the Community](https://discord.gg/llm-cost-ops)
