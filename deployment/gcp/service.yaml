# Cloud Run Service Definition for LLM-CostOps
# This is the unified service exposing all CostOps agents
#
# Service Name: llm-costops
# Agents Exposed:
#   - Cost Attribution Agent: /api/v1/agents/cost-attribution/*
#   - Cost Forecasting Agent: /api/v1/agents/cost-forecasting/*
#   - Budget Enforcement Agent: /api/v1/agents/budget-enforcement/*
#   - ROI Estimation Agent: /api/v1/agents/roi-estimation/*
#   - Cost-Performance Tradeoff Agent: /api/v1/agents/cost-performance/*

apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: llm-costops
  namespace: default
  labels:
    app: llm-costops
    component: finops-governance
    platform: agentics
  annotations:
    run.googleapis.com/ingress: internal
    run.googleapis.com/launch-stage: GA
spec:
  template:
    metadata:
      annotations:
        # Autoscaling configuration
        autoscaling.knative.dev/minScale: "1"
        autoscaling.knative.dev/maxScale: "100"
        autoscaling.knative.dev/target: "80"

        # VPC Connector for internal network access
        run.googleapis.com/vpc-access-connector: projects/agentics-dev/locations/us-central1/connectors/agentics-vpc-connector
        run.googleapis.com/vpc-access-egress: private-ranges-only

        # Cloud SQL Proxy (NOT used - all persistence via ruvector-service)
        # Explicitly NOT connecting to Cloud SQL

        # Startup probe
        run.googleapis.com/startup-cpu-boost: "true"

      labels:
        app: llm-costops
        version: "${SERVICE_VERSION}"
    spec:
      containerConcurrency: 100
      timeoutSeconds: 60
      serviceAccountName: llm-costops-sa@agentics-dev.iam.gserviceaccount.com

      containers:
        - name: llm-costops
          image: us-central1-docker.pkg.dev/agentics-dev/llm-devops/llm-costops:latest

          ports:
            - name: http1
              containerPort: 8080

          resources:
            limits:
              cpu: "2"
              memory: 1Gi
            requests:
              cpu: "1"
              memory: 512Mi

          env:
            # Required: Platform Environment
            - name: PLATFORM_ENV
              value: "${PLATFORM_ENV}"

            # Required: Service Identity
            - name: SERVICE_NAME
              value: llm-costops
            - name: SERVICE_VERSION
              value: "${SERVICE_VERSION}"

            # Required: RuVector Service (persistence layer)
            - name: RUVECTOR_SERVICE_URL
              value: "https://ruvector-service-${PLATFORM_ENV}.agentics.internal"
            - name: RUVECTOR_API_KEY
              valueFrom:
                secretKeyRef:
                  name: ruvector-api-key
                  key: latest

            # Required: Telemetry (LLM-Observatory)
            - name: TELEMETRY_ENDPOINT
              value: "https://llm-observatory-${PLATFORM_ENV}.agentics.internal/v1/ingest"
            - name: TELEMETRY_API_KEY
              valueFrom:
                secretKeyRef:
                  name: observatory-api-key
                  key: latest

            # Logging configuration
            - name: RUST_LOG
              value: "info,llm_cost_ops=debug"
            - name: LOG_FORMAT
              value: "json"

            # Server configuration
            - name: PORT
              value: "8080"
            - name: METRICS_PORT
              value: "9090"

            # Request timeouts
            - name: REQUEST_TIMEOUT_SECS
              value: "30"
            - name: RUVECTOR_TIMEOUT_SECS
              value: "10"

          # Startup probe - wait for service to be ready
          startupProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 3
            failureThreshold: 12

          # Liveness probe - restart if unhealthy
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 15
            timeoutSeconds: 5
            failureThreshold: 3

          # Readiness probe - only receive traffic when ready
          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 3
            failureThreshold: 3

  traffic:
    - percent: 100
      latestRevision: true
